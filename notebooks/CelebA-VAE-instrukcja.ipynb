{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../materials/title.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color='#58ACFA'> AI Workshop - Część I</font></center>\n",
    "## <center>Wykorzystanie generatywnych autoenkoderów do ekstrakcji cech istotnych w procesie wyszukiwania podobnych twarzy</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#58ACFA'>Plan pracy:</font>\n",
    "\n",
    "#### <font color='#0B3861'> >>> Zbudujemy autoenkoder wariacyjny zdolny do identyfikacji charakterystycznych cech twarzy.</font>\n",
    "#### <font color='#0B3861'> >>> Wykorzystamy zbudowany autoenkoder do odnajdywania podobnych twarzy.</font>\n",
    "#### <font color='#0B3861'> >>> Stworzymy nowe twarze :D</font>\n",
    "\n",
    "\n",
    "### <font color='#0B3861'> Do tego celu posłużymy się danymi pochodzącymi ze zbioru CelebA. Zbiór ten posiada ponad 200 tysięcy zdjęć twarzy znanych celebrytów.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../materials/dataset.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Uwaga: Miejsca w kodzie do uzupełnienia są oznaczone poprzez \"____\"</font>\n",
    "\n",
    "## <font color='#0B3861'>Zaczniemy od zaimportowania niezbędnych bibliotek i ustawienia odpowiednich folderów</font>\n",
    "##### Wykorzystane funkcje zostaną wytłumaczone w dalszej części notatnika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user -r ../requirements.txt # pobieranie bibliotek z pliku requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# operacje związane ze ścieżkami\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# operacje na macierzach\n",
    "import numpy as np\n",
    "\n",
    "# operacje na obrazach\n",
    "from PIL import Image\n",
    "\n",
    "# wizualizacja postępu wykonania kodu\n",
    "from tqdm import tqdm\n",
    "\n",
    "# tworzenie wykresów i wizualizacja danych\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# importy związane z budową i treningiem modelu\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Input \n",
    "from keras.layers import Conv2D, Flatten, Lambda\n",
    "from keras.layers import Reshape, Conv2DTranspose\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import mse, kullback_leibler_divergence\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# importy związane z wyszukiwaniem podobnych twarzy\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#0B3861'>Pobranie i rozpakowanie danych</font>\n",
    "\n",
    "*   <font color='red'>Linux</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/lpmjzzk26nae9bh/img_align_celeba.zip\n",
    "!unzip -qq img_align_celeba.zip -d .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Windows  \n",
    "Wejdź na stronę: https://www.dropbox.com/s/lpmjzzk26nae9bh/img_align_celeba.zip\n",
    "Pobierz plik i wypakuj w utworzonym folderze (*img_align_celeba*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tworzymy foldery na zdjęcia i modele\n",
    "dataset_dir = '../img_align_celeba'\n",
    "model_dir = '../models'\n",
    "\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zawsze warto ustawić tak zwane ziarno - określa ono punkt początkowy dla algorytmu generacji liczb pseudolosowych. Dzięki temu przy każdorazowym uruchomieniu notatnika uzyskamy te same wyniki. \n",
    "Podpowiedź: ziarno ustawiamy podając dowolną liczbę do funkcji <font color=red>np.random.seed()</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ustaw swoje ziarno\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#0B3861'>Wczytanie i przygotowanie danych</font>\n",
    "#### Dane powinny znajdować się w folderze *dataset_dir*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = glob(dataset_dir + \"/*.jpg\") # pobierze ścieżki do każdego z obrazów w folderze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wszystkich obrazów powinno być 202599. Sprawdź, czy rzeczywiście tak jest i nic się nie zgubiło :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wyświetl ilość obrazów wczytanych do notatnika\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### W celu przyspieszenia obliczeń wykorzystamy jedynie 10 000 losowo wybranych obrazów. Potrzebujemy zatem 10 000 losowo wybranych indeksów. Pozwala na to funkcja <font color=red>np.random.choice()</font>, która przyjmuje dwa argumenty: pierwszy określa elementy z jakich losujemy (w naszym przypadku będzie to liczba wszystkich obrazów), a drugi to ilość liczb jakie zostaną zwrócone\n",
    "##### *ustawione wcześniej ziarno sprawi, że zawsze wylosujemy ten sam zestaw liczb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wygeneruj 10000 losowych indeksów\n",
    "rnd_choice = _____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Razem z wczytaniem danych  (<font color='red'>Image.open()</font>) przygotujemy je do wykorzystania: przytniemy krawędzie (<font color='red'>crop()</font>) i zmniejszymy ich rozmiar (<font color='red'>resize()</font>), co znacznie przyspieszy trening modelu.\n",
    "#### Zastosuj wylosowane indeksy w celu wczytania zbioru danych do pracy. Przytnij obraz wprowadzając wartości 9, 13, 169 i 205 (koordynaty pikseli) w odpowednie miejsce. Następnie zmniejsz obraz do rozmiaru 80x96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(\n",
    "                [np.array(Image.open(img_paths[x]).crop(box=(____)).resize((____)))\n",
    "                 for x in tqdm(rnd_choice.tolist())]\n",
    "                ).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Żeby upewnić się, że wszystko poszło dobrze, wyświetl wymiary obrazów "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dokończ skalowanie obrazów do wartości z przedziału [0,1], w każdym z kanałów RGB (3), wykorzystując funkcje biblioteki numpy (<font color='red'>np.min(), np.max()</font>) oraz poniższy wzór:\n",
    "\\begin{equation}\n",
    "X = \\frac{X - min(X)}{max(X) - min(X)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    images[:,:,:,i] = (____ - ____)/(____ - ____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upewnij się, że z danymi wszystko w porządku zanim przejdziesz do ich analizy. Wyświetl przykładowe 9 obrazów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(1,10):\n",
    "    plt.subplot(3,3,i)\n",
    "    plt.imshow(np.squeeze(images[i]))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#0B3861'>Budowa modelu wariacyjnego autoenkodera (VAE)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Na początku zdefiniujemy parametry określające model VAE i jego trening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Parametry określające model \"\"\"\n",
    "kernel_size = 3 # zwyczajowa wielkość fitra konwolucyjnego\n",
    "filters = 16 # liczba filtrów dla pierwszej warstwy konwolucyjnej, rośnie 2-krotnie dla następnych\n",
    "layers = 3 # ilość warstw konwolucyjnych tworzących enkoder i dekoder\n",
    "input_shape = images.shape[1:4] # wielkość obrazu wejściowego, wykorzystywana dla budowy grafu SN\n",
    "n_dense = 256 # liczba neuronów dla środkowej warstwy ukrytej - perceptronu\n",
    "latent_dim = 32 # liczba cech które uzyskamy z enkodera\n",
    "\n",
    "\"\"\" Parametry określające trening sieci \"\"\"\n",
    "batch_size = 64 # liczba obrazów wykorzystywana do jednego kroku treningu SN\n",
    "optimizer = 'adam' # funkcja odpowiadająca za redukcję błędu sieci\n",
    "epochs = 30 # maksymalna liczba epok, czyli ile razy sieć będzie widziała dane treningowe\n",
    "model_path = os.path.join(model_dir,'celeb_vae_v2.h5') # ustawienie ścieżki do pliku z modelem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zdefiniujemy niezbędne funkcje: <font color='red'>sampling()</font> generującą nowe dane w warstwie ukrytej *z* oraz <font color='red'>vae_loss()</font> będącą funkcją straty wykorzystującą błąd MAE jak i dywergencję Kullbacka-Leiblera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    \"\"\" Funkcja generująca wartości ze standardowych rozkładów normalnych \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss():\n",
    "    \"\"\" Funkcja strat (MSE + Kullback-Leibler) \"\"\"\n",
    "    \n",
    "    reconstruction_loss = mse(K.flatten(inputs), K.flatten(outputs))\n",
    "    reconstruction_loss *= input_shape[0] * input_shape[1]\n",
    "    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "    kl_loss = K.sum(kl_loss, axis=-1)\n",
    "    kl_loss *= -0.5\n",
    "    vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "    \n",
    "    return vae_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Na podstawie poniższego schematu i wskazówek zbuduj model wariacyjnego autoenkodera (uzupełnij brakujące części kodu) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../materials/vae.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../materials/vae_legenda.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'> Ważne wskazówki </font>:\n",
    "*   Zwróć uwagę na kolory warstw na powyższym schemacie i nazwy w nawiasach\n",
    "*   Brakujące warstwy uzupełniaj analogicznie do wpisanych już przykładów\n",
    "*   Uzupełnij brakujące pola \"____\" w wyznaczonych miejscach\n",
    "*   Funkcja aktywacji w warstwach konwolucyjnych to <font color='red'>'relu'</font>\n",
    "*   Dla każdej kolejnej warstwy konwolucyjnej (W ENKODERZE!) liczbę filtrów <font color='red'>zwiększ dwukrotnie w stosunku do poprzedniej</font>\n",
    "*   Dla każdej kolejnej warstwy konwolucyjnej transponowanej (W DEKODERZE!) liczbę filtrów <font color='red'>zmniejsz dwukrotnie w stosunku do poprzedniej</font>\n",
    "*   Stosujemy padding z zachowaniem wymiarów (<font color='red'>'same'</font>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENKODER\n",
    "\n",
    "inputs = Input(shape=input_shape, name='encoder_input') # stworzenie wejścia sieci neuronowej\n",
    "x = inputs\n",
    "\n",
    "x = Conv2D(filters=filters*2, kernel_size=kernel_size, \n",
    "           activation='relu', strides=2, padding='same')(x) #pierwsza warstwa konwolucyjna enkodera\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TU DODAJ BRAKUJĄCE WARSTWY: x = ___ itp, ze zwiększoną DWUKROTNIE liczbą filtrów w stosunku do poprzedniej warstwy (*4, *8)\n",
    "x = ____(____)(x)\n",
    "x = ____(____)(x)\n",
    "\n",
    "\n",
    "\n",
    "shape = K.int_shape(x) # zapisanie wielkości wyjścia warstwy do późniejszego stworzenia dekodera\n",
    "\n",
    "x = ____()(x) #SPŁASZCZENIE danych do formy wektora\n",
    "x = Dense(n_dense, activation=____)(x) #dodatkowa pośrednia warstwa pełna\n",
    "\n",
    "# trenowalne średnie oraz wariancje do celu generacji wektora ukrytego\n",
    "z_mean = ____(latent_dim, name='z_mean')(x)\n",
    "z_log_var = _____(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "\n",
    "z = ____(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var]) # wartości wyjściowe z enkodera\n",
    "\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder') # stworzenie enkodera\n",
    "\n",
    "\n",
    "\n",
    "# DEKODER\n",
    "\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling') # stworzenie wejścia dla dekodera\n",
    "\n",
    "\n",
    "x = ____(n_dense, activation=____)(latent_inputs) # pośrednia warstwa pełna\n",
    "\n",
    "\n",
    "x = Dense(shape[1] * shape[2] * shape[3], activation=____)(x)\n",
    "x = Reshape((shape[1], shape[2], shape[3]))(x) # wektor -> macierz\n",
    "\n",
    "\n",
    "x = Conv2DTranspose(filters=filters*4, kernel_size=____, \n",
    "                    activation=____, strides=2, padding=____)(x) #pierwsza warstwa konwolucyjna dekodera\n",
    "\n",
    "\n",
    "\n",
    "# TU DODAJ BRAKUJĄCE WARSTWY: x = ___ itp, ze zmniejszoną DWUKROTNIE liczbą filtrów w stosunku do poprzedniej warstwy (*2, *1)\n",
    "x = ____(____)(x)\n",
    "x = ____(____)(x)\n",
    "\n",
    "\n",
    "outputs = _____(filters=3, kernel_size=kernel_size, activation='sigmoid', \n",
    "                padding='same', name='decoder_output')(x) # warstwa wyjściowa\n",
    "\n",
    "\n",
    "decoder = Model(____, ____, name='decoder') # stworzenie dekodera (wpisz dane wejściowe i wyjściowe z dekodera)\n",
    "\n",
    "\n",
    "# stworzenie modelu VAE\n",
    "\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#0B3861'>Trening modelu wariacyjnego autoenkodera (VAE)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przygotowanie do treningu\n",
    "#### Podział danych na treningowe i testowe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(images, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wprowadzenie parametrów treningowych i funkcji monitorujących przebieg treningu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.add_loss(vae_loss())\n",
    "vae.compile(optimizer=optimizer)\n",
    "\n",
    "early_stop = EarlyStopping(restore_best_weights=True, patience=3) # zatrzyma uczenie jeśli nie będzie poprawy przez 3 epoki\n",
    "checkpt = ModelCheckpoint(model_path, save_best_only=True, save_weights_only=True) # zapisze wagi dla najlepszej epoki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = vae.fit(X_train,\n",
    "                  epochs=epochs,\n",
    "                  batch_size=batch_size,\n",
    "                  validation_data=(X_test, None),\n",
    "                  verbose=2,\n",
    "                  callbacks = [early_stop, checkpt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#0B3861'>Weryfikacja rezultatów</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wczytanie zapisanych wag z epoki, dla której wartość funkcji straty była najniższa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jeśli zastosujemy .summary() na utworzonym modelu, uzyskamy informacje o jego budowie. Wyświetl budowę enkodera oraz dekodera oraz odpowiedz na pytania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(____.summary()) # budowa enkodera\n",
    "\n",
    "print(____) # budowa dekodera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jak zmienił się kształt obrazów wejściowych po pierwszej warstwie konwolucyjnej enkodera? : _____\n",
    "#### Jaka jest całkowita liczba trenowalnych parametrów w dekoderze? : _____\n",
    "#### Jakie są wymiary filtrów wychodzących z drugiej warstwy konwolucyjnej transponowanej dekodera? : _____\n",
    "#### Dlaczego w podanych wymiarach na każdej z warstw na pierwszej pozycji pojawia się None? : _____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dokonaj predykcji danych testowych na enkoderze i dekoderze (wyjściowa macierz <font color='red'>z</font> z enkodera jest wejściem do dekodera):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mean, z_std_var, z = encoder.predict(X_test)\n",
    "\n",
    "y_pred = decoder.____(____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wyświetl wymiary wyjściowej macierzy z. Czym są wiersze a czym kolumny tej macierzy ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wymiary macierzy z:\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wyświetlenie zdjęć oryginalnych i wyjściowych z autoenkodera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(10,2, figsize = (10,45))\n",
    "for i in range(10):\n",
    "    ax[i,0].imshow(np.squeeze(X_test[i]))\n",
    "    ax[i,0].set_title('Prawdziwe zdjęcie')\n",
    "    ax[i,0].axis('off')\n",
    "    ax[i,1].imshow(np.squeeze(y_pred[i]))\n",
    "    ax[i,1].set_title('Wyjście z VAE')\n",
    "    ax[i,1].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#0B3861'>Wyszukiwanie twarzy najbardziej do siebie podobnych</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do wybranego przez siebie zdjęcia twarzy odnajdź 5 najbardziej podobnych testowych przykładów.\n",
    "#### Wybierz zdjęcie i wyświetl je:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wybierz jeden obraz z danych testowych (X_test) poprzez indeksowanie\n",
    "\n",
    "index = ____ # wybierz numer przykładu\n",
    "ref = ____ # wyciągnij obraz o indeksie 'index' z danych testowych\n",
    "\n",
    "plt.imshow(np.squeeze(ref))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dla wybranego obrazu wyświetl jego wersję wychodzącą z dekodera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ref = y_pred[index]\n",
    "\n",
    "plt.imshow(np.squeeze(y_pred_ref))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wylicz odległość pomiędzy wybranym obrazem a pozostałymi przykładami testowymi. Wyliczony wektor odległości posortuj rosnąco i wybierz 5 najniższych indeksów - oznaczających najbardziej podobne przykłady do wybranego przez ciebie\n",
    "\n",
    "#### Odległość jest wyliczana pomiędzy wyjściowymi wektorami cech z enkodera (odległość między wektorami <font color='red'>z</font> wybranego obrazu a pozostałych przykładów). Wykorzystamy odległość euklidesową (<font color='red'>euclidean</font>), ale możesz poeksperymentować z innymi metrykami (np. manhattan, correlation, sqeuclidean czy cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_ref = z[index]\n",
    "z_ref = z_ref.reshape(1, -1) # konwersja na macierz z jedną obserwacją (1 wiersz)\n",
    "\n",
    "distances = pairwise_distances(____, _____, metric='____')\n",
    "\n",
    "# distances to macierz zawierająca obliczone odległości pomiędzy wybranym obrazem (wektor cech) a każdym przykładem testowym\n",
    "print(distances.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mając policzoną macierz odległości, posortuj ją rosnąco i wybierz 5 pierwszych indeksów. Wykorzystaj do tego celu funkcję z biblioteki numpy <font color='red'>np.argsort()</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_sorted = _____(____)[0] # uzyskaj indeksy posortowanych wartości\n",
    "\n",
    "closest = indices_sorted[:____] # wybierz 5 najbliższych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wyświetl odnalezione, najbardziej podobne obrazy\n",
    "#### Najbliższe obrazy uzyskane z dekodera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in closest:\n",
    "    plt.imshow(np.squeeze(y_pred[i]))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Najbliższe obrazy oryginalne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in closest:\n",
    "    plt.imshow(np.squeeze(X_test[i]))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dyskusja:\n",
    "### <font color='#58ACFA'>>>> Które obrazy są do siebie bardziej podobne (uzyskane z dekodera czy oryginalne wersje)?</font>\n",
    "### <font color='#58ACFA'>>>> Co jest przyczyną takiego rezultatu ?</font>\n",
    "### <font color='#58ACFA'>>>> Czy podobieństwa pomiędzy obrazami są wyraźnie widoczne ? Jeśli nie, jaka może być tego przyczyna ?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#0B3861'>Generowanie nowych twarzy</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE to model generatywny, co znaczy, że pozwala na stworzenie nowych, nieistniejących przykładów\n",
    "#### Polega to na tym, że zamiast wprowadzać do dekodera wyjściowy wektor cech z enkodera, wprowadzamy wektory z losowo wybranymi liczbami próbkowanymi z rozkładu standardowego (średnia równa 0, odchylenie standardowe równe 1)\n",
    "#### Możemy to osiągnąć za pomoca funkcji <font color='red'>np.random.randn()</font>, która zwraca losowe liczby z rozkładu N(0,1). Funkcja przyjmuje wartości określające rozmiar generowanej macierzy. W naszym przypadku musi być to wektor o długości wektora cech, wychodzącego z enkodera, dla jednego obrazu (jeden wiersz, ilość cech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5,4, figsize = (10,10))\n",
    "for i in range(5):\n",
    "    for j in range(4):\n",
    "        z_generated = np.random.randn(____,____) # stwórz wektor losowych liczb o długości wektora cech\n",
    "        y_pred_generated = ____.predict(____) # uzyskaj predykcję z dekodera na wygenerowanym wektorze\n",
    "        ax[i,j].imshow(np.squeeze(y_pred_generated))\n",
    "        ax[i,j].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Czy wygenerowane twarze wyglądają na rzeczywiste? :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
